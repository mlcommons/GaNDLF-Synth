{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GaNDLF-Synth \u00b6 The G ener a lly N uanced D eep L earning F ramework - Synth esis (GaNDLF-Synth) for reproducible and automated deep generative modeling in medical imaging. Why use GaNDLF-Synth? \u00b6 GaNDLF-Synth was developed to lower the barrier to AI, enabling reproducibility, translation, and deployment regarding usage of generative models in medical imaging. It is an extension of the GaNDLF framework, which is a part of the MLCommons initiative. GaNDLF-Synth aims to extend the capabilities of GaNDLF to include generative models, such as GANs, VAEs, and diffusion models, while adhering to the same principles. As an out-of-the-box solution, GaNDLF alleviates the need to build from scratch. Users may kickstart their project by modifying only a configuration (config) file that provides guidelines for the envisioned pipeline and CSV inputs that describe the training data. Range of GaNDLF-Synth functionalities: \u00b6 Supports multiple Deep Generative model architectures Channels/modalities Labeling schemes (per patient, per custom class, unlabeled) Support of multiple loss, optimizers, scheduler, data augmentation, and evaluation metrics via interfacing GaNDLF Multi-GPU and multi-node training and inference support, integrating DistributedDataParallel DDP and deepspeed ; parallelism present both on the model and data level Leverages robust open source software - Pytorch Lightning , monai-generative Zero -code needed to train robust models and generate synthetic data Low -code requirement for customization and addition of custom models and training logic Automatic mixed precision support Table of Contents \u00b6 Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF FAQ Acknowledgements Citation \u00b6 Please cite the following article for GaNDLF-Synth: @misc { pati2024gandlfsynthframeworkdemocratizegenerative , title = {GaNDLF-Synth: A Framework to Democratize Generative AI for (Bio)Medical Imaging} , author = {Sarthak Pati and Szymon Mazurek and Spyridon Bakas} , year = {2024} , eprint = {2410.00173} , archivePrefix = {arXiv} , primaryClass = {cs.LG} , url = {https://arxiv.org/abs/2410.00173} , } Contact \u00b6 GaNDLF developers can be reached via the following ways: - GitHub Discussions - Email","title":"Home"},{"location":"#gandlf-synth","text":"The G ener a lly N uanced D eep L earning F ramework - Synth esis (GaNDLF-Synth) for reproducible and automated deep generative modeling in medical imaging.","title":"GaNDLF-Synth"},{"location":"#why-use-gandlf-synth","text":"GaNDLF-Synth was developed to lower the barrier to AI, enabling reproducibility, translation, and deployment regarding usage of generative models in medical imaging. It is an extension of the GaNDLF framework, which is a part of the MLCommons initiative. GaNDLF-Synth aims to extend the capabilities of GaNDLF to include generative models, such as GANs, VAEs, and diffusion models, while adhering to the same principles. As an out-of-the-box solution, GaNDLF alleviates the need to build from scratch. Users may kickstart their project by modifying only a configuration (config) file that provides guidelines for the envisioned pipeline and CSV inputs that describe the training data.","title":"Why use GaNDLF-Synth?"},{"location":"#range-of-gandlf-synth-functionalities","text":"Supports multiple Deep Generative model architectures Channels/modalities Labeling schemes (per patient, per custom class, unlabeled) Support of multiple loss, optimizers, scheduler, data augmentation, and evaluation metrics via interfacing GaNDLF Multi-GPU and multi-node training and inference support, integrating DistributedDataParallel DDP and deepspeed ; parallelism present both on the model and data level Leverages robust open source software - Pytorch Lightning , monai-generative Zero -code needed to train robust models and generate synthetic data Low -code requirement for customization and addition of custom models and training logic Automatic mixed precision support","title":"Range of GaNDLF-Synth functionalities:"},{"location":"#table-of-contents","text":"Getting Started Application Setup Usage Customize the training and inference Extending GaNDLF FAQ Acknowledgements","title":"Table of Contents"},{"location":"#citation","text":"Please cite the following article for GaNDLF-Synth: @misc { pati2024gandlfsynthframeworkdemocratizegenerative , title = {GaNDLF-Synth: A Framework to Democratize Generative AI for (Bio)Medical Imaging} , author = {Sarthak Pati and Szymon Mazurek and Spyridon Bakas} , year = {2024} , eprint = {2410.00173} , archivePrefix = {arXiv} , primaryClass = {cs.LG} , url = {https://arxiv.org/abs/2410.00173} , }","title":"Citation"},{"location":"#contact","text":"GaNDLF developers can be reached via the following ways: - GitHub Discussions - Email","title":"Contact"},{"location":"acknowledgements/","text":"Acknowledgements \u00b6 This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis. Papers \u00b6 Application Lead Author Link Framework introduction Sarthak Pati Arxiv People \u00b6 All coders and developers of GaNDLF-Synth Supervisors: Spyridon Bakas","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"This file records the following pieces of information: All papers whose results were reproduced for the GaNDLF manuscript . All challenges (links and references) that various developers/users participated in with GaNDLF. All papers that used GaNDLF in their analysis.","title":"Acknowledgements"},{"location":"acknowledgements/#papers","text":"Application Lead Author Link Framework introduction Sarthak Pati Arxiv","title":"Papers"},{"location":"acknowledgements/#people","text":"All coders and developers of GaNDLF-Synth Supervisors: Spyridon Bakas","title":"People"},{"location":"customize/","text":"This file contains mid-level information regarding various parameters that can be leveraged to customize the training/inference in GaNDLF. To see the default parameters for certain fields, see the default configs directory . Training Parameters \u00b6 The training parameters are defined in the configuration file. The following fields are supported: model_config : # Configuration for the model (see below) - required modality : # Modality of the input data, either 'rad' or 'histo' - required num_epochs : # Number of epochs to train the model batch_size : # Batch size for training, validation and test dataloaders data_preprocessing : # Data preprocessing configuration (see below) data_augmentation : # Data augmentation configuration (see below) data_postprocessing : # Data postprocessing configuration (see below) dataloader_config : # Dataloaders configuration (see below) inference_parameters : batch_size : # Batch size for inference n_images_to_generate : # Number of images to generate during inference, unused in image-to-image models that require input images. This field can be a single value or a dictionary containing the number of images to generate for each class, for example {\"1\": 10, \"2\": 20}. save_model_every_n_epochs : # Save checkpoint every n epochs compute : {} # Distributed training and mixed precision configuration (see below) Model \u00b6 Model configuration is expected to be in the following format: model_config : model_name : # Name of the model to use labeling_paradigm : # Labeling paradigm for the model, either 'unlabeled', 'patient', or 'custom'. Read in the \"Custom-Labels\" section for clarification on these three. architecture : # Architecture of the model, customizing given model. Specifics are defined in the config of the given model. losses : # Loss functions to use (see below). - name : # Name of the loss function - some_parameter : some_value # For models containing multiple losses (for example GANS), the losses are expected to be in the following format: losses : - discriminator : # Discriminator loss - name : # Name of the loss function - some_parameter : some_value - generator : # Generator loss - name : # Name of the loss function - some_parameter : some_value # Note that to use multiple losses, the model should be prepared via config to handle it via certain subloss name. - optimizers : # Optimizers to use (see below) - name : # Name of the optimizer - some_parameter : some_value # For models containing multiple optimizers (for example GANS), the optimizers can be defined as losses above. - schedulers : # Schedulers to use (see below) - name : # Name of the scheduler - some_parameter : some_value # For models containing multiple schedulers (for example GANS), the schedulers can be defined as losses above. - n_channels : # Number of input channels - n_dimensions : # Number of dimensions of the input data (2 or 3) - tensor_shape : # Shape of the input tensor # This model config can support additional parameters that are specific to the model, for example: - save_eval_images_every_n_epochs : # Save evaluation images every n epochs, useful to assess training progress of generative models. Implemented in i.e. DCGAN. Regarding the \"labeling_paradigm\" Custom Labels \u00b6 Custom labels are defined based on the folder structure specified by the user. For example, you can create directories such as 0 , 1 , 2 , etc., where each number represents a distinct class. These classes are arbitrary and can be assigned as per the user's requirements. Inside each class folder, you should organize patient-specific subfolders containing the corresponding data. Example Structure: /data \u251c\u2500\u2500 0 \u2502 \u251c\u2500\u2500 patient_001 \u2502 \u2514\u2500\u2500 patient_002 \u251c\u2500\u2500 1 \u2502 \u251c\u2500\u2500 patient_003 \u2502 \u2514\u2500\u2500 patient_004 \u2514\u2500\u2500 2 \u251c\u2500\u2500 patient_005 \u2514\u2500\u2500 patient_006 Patient-Level Labels \u00b6 For patient-level labels, the folder structure consists of a main directory where each subfolder represents a specific patient. In this setup, each patient is treated as a separate class, similar to how labeling is handled in GANDLF. Example Structure: /data \u251c\u2500\u2500 patient_001 \u251c\u2500\u2500 patient_002 \u251c\u2500\u2500 patient_003 \u2514\u2500\u2500 patient_004 Unlabeled Data \u00b6 For unlabeled data, you are required to maintain a patient-wise folder structure, similar to the patient-level labeling setup. However, in this case, class labels are ignored. The data is simply loaded without any label association. Example Structure: /data \u251c\u2500\u2500 patient_001 \u251c\u2500\u2500 patient_002 \u251c\u2500\u2500 patient_003 \u2514\u2500\u2500 patient_004 Optimizers \u00b6 GaNDLF-Synth interfaces GaNDLF core framework for optimizers. See the optimizers directory for available optimizers. They support optimizer-specific configurable parameters, interfacing Pytorch Optimizers . Schedulers \u00b6 GaNDLF-Synth interfaces GaNDLF core framework for schedulers. See the schedulers directory for available schedulers. They support scheduler-specific configurable parameters, interfacing Pytorch Schedulers . Losses \u00b6 GaNDLF-Synth supports multiple loss functions. See the losses directory for available loss functions. They support loss-specific configurable parameters, interfacing Pytorch Loss functions . Dataloader configs \u00b6 GaNDLF-Synth supports separate dataloader parameters for training, validation, test and inference dataloaders. They support configurable parameters, interfacing Pytorch Dataloader . The following fields are supported: dataloader_config : # Dataloaders configuration (see below) shuffle : # Whether to shuffle the data num_workers : # Number of processes spawned to load the data pin_memory : # Whether to pin the memory for CPU-GPU transfer timeout : # Timeout for the dataloader processes prefetch_factor : # Number of batches to prefetch by each worker persistent_workers : # Whether to keep the worker processes alive between epochs The fields for specific dataloaders are expected to be in the following format: dataloader_config : train : - some_parameter : some_value val : - some_parameter : some_value test : - some_parameter : some_value inference : - some_parameter : some_value If given dataloader is not configured explicitly, the default values are used (see above). Data preprocessing \u00b6 GaNDLF-Synth interfaces GaNDLF core framework for data preprocessing. To see available data preprocessing options, see here . Separate preprocessing parameters can be defined for each dataloader (train, val, test, inference) as follows: data_preprocessing : train : - some_transform : some_value val : - some_transform : some_value test : - some_transform : some_value inference : - some_transform : some_value Data Augmentation \u00b6 GaNDLF-Synth interfaces GaNDLF core framework for data augmentation. To see available data augmentation options, see here Augmentations are applied only to the training dataloader. data_augmentation : train : - some_transform : some_value Post processing \u00b6 GaNDLF-Synth interfaces GaNDLF core framework for post processing. To see available post processing options, see here . Post-processing is applied only to the inference dataloader. data_postprocessing : inference : - some_transform : some_value Distributed Training \u00b6 For detalis on using distributed training, see the usage page .","title":"Customize training and inference"},{"location":"customize/#training-parameters","text":"The training parameters are defined in the configuration file. The following fields are supported: model_config : # Configuration for the model (see below) - required modality : # Modality of the input data, either 'rad' or 'histo' - required num_epochs : # Number of epochs to train the model batch_size : # Batch size for training, validation and test dataloaders data_preprocessing : # Data preprocessing configuration (see below) data_augmentation : # Data augmentation configuration (see below) data_postprocessing : # Data postprocessing configuration (see below) dataloader_config : # Dataloaders configuration (see below) inference_parameters : batch_size : # Batch size for inference n_images_to_generate : # Number of images to generate during inference, unused in image-to-image models that require input images. This field can be a single value or a dictionary containing the number of images to generate for each class, for example {\"1\": 10, \"2\": 20}. save_model_every_n_epochs : # Save checkpoint every n epochs compute : {} # Distributed training and mixed precision configuration (see below)","title":"Training Parameters"},{"location":"customize/#model","text":"Model configuration is expected to be in the following format: model_config : model_name : # Name of the model to use labeling_paradigm : # Labeling paradigm for the model, either 'unlabeled', 'patient', or 'custom'. Read in the \"Custom-Labels\" section for clarification on these three. architecture : # Architecture of the model, customizing given model. Specifics are defined in the config of the given model. losses : # Loss functions to use (see below). - name : # Name of the loss function - some_parameter : some_value # For models containing multiple losses (for example GANS), the losses are expected to be in the following format: losses : - discriminator : # Discriminator loss - name : # Name of the loss function - some_parameter : some_value - generator : # Generator loss - name : # Name of the loss function - some_parameter : some_value # Note that to use multiple losses, the model should be prepared via config to handle it via certain subloss name. - optimizers : # Optimizers to use (see below) - name : # Name of the optimizer - some_parameter : some_value # For models containing multiple optimizers (for example GANS), the optimizers can be defined as losses above. - schedulers : # Schedulers to use (see below) - name : # Name of the scheduler - some_parameter : some_value # For models containing multiple schedulers (for example GANS), the schedulers can be defined as losses above. - n_channels : # Number of input channels - n_dimensions : # Number of dimensions of the input data (2 or 3) - tensor_shape : # Shape of the input tensor # This model config can support additional parameters that are specific to the model, for example: - save_eval_images_every_n_epochs : # Save evaluation images every n epochs, useful to assess training progress of generative models. Implemented in i.e. DCGAN. Regarding the \"labeling_paradigm\"","title":"Model"},{"location":"customize/#custom-labels","text":"Custom labels are defined based on the folder structure specified by the user. For example, you can create directories such as 0 , 1 , 2 , etc., where each number represents a distinct class. These classes are arbitrary and can be assigned as per the user's requirements. Inside each class folder, you should organize patient-specific subfolders containing the corresponding data. Example Structure: /data \u251c\u2500\u2500 0 \u2502 \u251c\u2500\u2500 patient_001 \u2502 \u2514\u2500\u2500 patient_002 \u251c\u2500\u2500 1 \u2502 \u251c\u2500\u2500 patient_003 \u2502 \u2514\u2500\u2500 patient_004 \u2514\u2500\u2500 2 \u251c\u2500\u2500 patient_005 \u2514\u2500\u2500 patient_006","title":"Custom Labels"},{"location":"customize/#patient-level-labels","text":"For patient-level labels, the folder structure consists of a main directory where each subfolder represents a specific patient. In this setup, each patient is treated as a separate class, similar to how labeling is handled in GANDLF. Example Structure: /data \u251c\u2500\u2500 patient_001 \u251c\u2500\u2500 patient_002 \u251c\u2500\u2500 patient_003 \u2514\u2500\u2500 patient_004","title":"Patient-Level Labels"},{"location":"customize/#unlabeled-data","text":"For unlabeled data, you are required to maintain a patient-wise folder structure, similar to the patient-level labeling setup. However, in this case, class labels are ignored. The data is simply loaded without any label association. Example Structure: /data \u251c\u2500\u2500 patient_001 \u251c\u2500\u2500 patient_002 \u251c\u2500\u2500 patient_003 \u2514\u2500\u2500 patient_004","title":"Unlabeled Data"},{"location":"customize/#optimizers","text":"GaNDLF-Synth interfaces GaNDLF core framework for optimizers. See the optimizers directory for available optimizers. They support optimizer-specific configurable parameters, interfacing Pytorch Optimizers .","title":"Optimizers"},{"location":"customize/#schedulers","text":"GaNDLF-Synth interfaces GaNDLF core framework for schedulers. See the schedulers directory for available schedulers. They support scheduler-specific configurable parameters, interfacing Pytorch Schedulers .","title":"Schedulers"},{"location":"customize/#losses","text":"GaNDLF-Synth supports multiple loss functions. See the losses directory for available loss functions. They support loss-specific configurable parameters, interfacing Pytorch Loss functions .","title":"Losses"},{"location":"customize/#dataloader-configs","text":"GaNDLF-Synth supports separate dataloader parameters for training, validation, test and inference dataloaders. They support configurable parameters, interfacing Pytorch Dataloader . The following fields are supported: dataloader_config : # Dataloaders configuration (see below) shuffle : # Whether to shuffle the data num_workers : # Number of processes spawned to load the data pin_memory : # Whether to pin the memory for CPU-GPU transfer timeout : # Timeout for the dataloader processes prefetch_factor : # Number of batches to prefetch by each worker persistent_workers : # Whether to keep the worker processes alive between epochs The fields for specific dataloaders are expected to be in the following format: dataloader_config : train : - some_parameter : some_value val : - some_parameter : some_value test : - some_parameter : some_value inference : - some_parameter : some_value If given dataloader is not configured explicitly, the default values are used (see above).","title":"Dataloader configs"},{"location":"customize/#data-preprocessing","text":"GaNDLF-Synth interfaces GaNDLF core framework for data preprocessing. To see available data preprocessing options, see here . Separate preprocessing parameters can be defined for each dataloader (train, val, test, inference) as follows: data_preprocessing : train : - some_transform : some_value val : - some_transform : some_value test : - some_transform : some_value inference : - some_transform : some_value","title":"Data preprocessing"},{"location":"customize/#data-augmentation","text":"GaNDLF-Synth interfaces GaNDLF core framework for data augmentation. To see available data augmentation options, see here Augmentations are applied only to the training dataloader. data_augmentation : train : - some_transform : some_value","title":"Data Augmentation"},{"location":"customize/#post-processing","text":"GaNDLF-Synth interfaces GaNDLF core framework for post processing. To see available post processing options, see here . Post-processing is applied only to the inference dataloader. data_postprocessing : inference : - some_transform : some_value","title":"Post processing"},{"location":"customize/#distributed-training","text":"For detalis on using distributed training, see the usage page .","title":"Distributed Training"},{"location":"extending/","text":"Environment \u00b6 Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF-Synth from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> gandlf-synth verify-install Submodule flowcharts \u00b6 The following flowcharts are intended to provide a high-level overview of the different submodules in GaNDLF-Synth. Navigate to the README.md file in each submodule folder for details. Some flowcharts are still in development and may not be complete/present. Overall Architecture \u00b6 Command-line parsing: gandlf-synth run Config Manager : Handles configuration parsing Provides configuration to other modules Training Manager : Main entry point from CLI Handles training functionality Inference Manager : Handles inference functionality Main entry point from CLI Performs actual inference Dependency Management \u00b6 To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF-Synth (see the python_requires variable in setup ). It does not clash with any existing dependencies. Adding Models \u00b6 Create the model in the new file in architectures folder. Make sure the new class inherits from ModelBase class. Create new LightningModule that will implement the training, validation, testing and inference logic. Make sure it inherits from [SynthesisModule] (https://github.com/mlcommons/GaNDLF-Synth/blob/main/gandlf_synth/models/module_abc.py) class and implements necessary abstract methods. Add the new model to the ModuleFactory AVAILABE_MODULES dictionary. Note that the key in this dictionary should follow the naming convention: labeling-paradigm_model-name , and needs to match the key in the model config factory that we will create in the next steps. Create the model config file in the configs folder. Implement the model configuration class that will parse model's configuration when creating model instance. Make sure it inherits from AbstractModelConfig class. Add the new model configuration class to the ModelConfigFactory AVAILABLE_MODEL_CONFIGS dictionary. Note that the model config key in this dictionary should follow the naming convention: labeling-paradigm_model-name , and needs to match the key in the model factory that we created in the previous steps. Adding Dataloading Functionalities \u00b6 GaNDLF-Synth handles dataloading for specific labeling paradigms in separate abstractions. If you wish to modify the dataloading functionality, please refer to the following modules: Datasets : Contains the dataset classes for different labeling paradigms. Dataset Factories : Contains the factory class that creates the dataset instance based on the configuration. Dataloaders Factories : Contains the factory class that creates the dataloader instance based on the configuration. Remember to add new datasets and dataloaders to the respective factory classes. For some cases, modifications of the training or inference logic may be required to accommodate the new dataloading functionality (see below). Adding Training Functionality \u00b6 For changes at the level of single training, validation, or test steps, modify the specific functions of a given module. For changes at the level of the entire training loop, modify the Training Manager . The main training loop is handled via Trainer class of Pytorch Lightning - please refer to the Pytorch Lightning documentation for more details. Adding Inference Functionality \u00b6 For changes at the level of single inference steps, modify the specific functions of a given module. Note that for inference, special dataloaders are used to load the data in the required format. For changes at the level of the entire inference loop, modify the Inference Manager . The main inference loop is handled via Trainer class of Pytorch Lightning - please refer to the Pytorch Lightning documentation for more details. Adding new CLI command \u00b6 Example: gandlf-synth run CLI command - Implement function and wrap it with @click.command() + @click.option() - Add it to cli_subommands dict The command would be available under gandlf-synth your-subcommand-name CLI command. Update parameters \u00b6 For any new feature that is configurable via config, please ensure the corresponding option in the \"extending\" section of this documentation is added, so that others can review/use/extend it as needed. Update Tests \u00b6 Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples. Note that tests are split into different categories, each having its own file in the aforementioned folder: - test_modules.py : module-specific tests - test_generic.py : global features tests - entrypoints/ : tests for specific CLI commands Run Tests \u00b6 Prerequisites \u00b6 Tests are using sample data , which gets downloaded and prepared automatically when you run unit tests. Prepared data is stored at GaNDLF-Synth/testing/data/ automatically the first time test are ran. However, you may want to download & explore data by yourself. Unit tests \u00b6 Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file GaNDLF-Synth/testing/failures.log . Code coverage \u00b6 The code coverage for the unit tests can be obtained by the following command: # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest --device cuda ; coverage report -m","title":"Extending GaNDLF-Synth"},{"location":"extending/#environment","text":"Before starting to work on the code-level on GaNDLF, please follow the instructions to install GaNDLF-Synth from sources . Once that's done, please verify the installation using the following command: # continue from previous shell ( venv_gandlf ) $> # you should be in the \"GaNDLF\" git repo ( venv_gandlf ) $> gandlf-synth verify-install","title":"Environment"},{"location":"extending/#submodule-flowcharts","text":"The following flowcharts are intended to provide a high-level overview of the different submodules in GaNDLF-Synth. Navigate to the README.md file in each submodule folder for details. Some flowcharts are still in development and may not be complete/present.","title":"Submodule flowcharts"},{"location":"extending/#overall-architecture","text":"Command-line parsing: gandlf-synth run Config Manager : Handles configuration parsing Provides configuration to other modules Training Manager : Main entry point from CLI Handles training functionality Inference Manager : Handles inference functionality Main entry point from CLI Performs actual inference","title":"Overall Architecture"},{"location":"extending/#dependency-management","text":"To update/change/add a dependency in setup , please ensure at least the following conditions are met: The package is being actively maintained . The new dependency is being testing against the minimum python version supported by GaNDLF-Synth (see the python_requires variable in setup ). It does not clash with any existing dependencies.","title":"Dependency Management"},{"location":"extending/#adding-models","text":"Create the model in the new file in architectures folder. Make sure the new class inherits from ModelBase class. Create new LightningModule that will implement the training, validation, testing and inference logic. Make sure it inherits from [SynthesisModule] (https://github.com/mlcommons/GaNDLF-Synth/blob/main/gandlf_synth/models/module_abc.py) class and implements necessary abstract methods. Add the new model to the ModuleFactory AVAILABE_MODULES dictionary. Note that the key in this dictionary should follow the naming convention: labeling-paradigm_model-name , and needs to match the key in the model config factory that we will create in the next steps. Create the model config file in the configs folder. Implement the model configuration class that will parse model's configuration when creating model instance. Make sure it inherits from AbstractModelConfig class. Add the new model configuration class to the ModelConfigFactory AVAILABLE_MODEL_CONFIGS dictionary. Note that the model config key in this dictionary should follow the naming convention: labeling-paradigm_model-name , and needs to match the key in the model factory that we created in the previous steps.","title":"Adding Models"},{"location":"extending/#adding-dataloading-functionalities","text":"GaNDLF-Synth handles dataloading for specific labeling paradigms in separate abstractions. If you wish to modify the dataloading functionality, please refer to the following modules: Datasets : Contains the dataset classes for different labeling paradigms. Dataset Factories : Contains the factory class that creates the dataset instance based on the configuration. Dataloaders Factories : Contains the factory class that creates the dataloader instance based on the configuration. Remember to add new datasets and dataloaders to the respective factory classes. For some cases, modifications of the training or inference logic may be required to accommodate the new dataloading functionality (see below).","title":"Adding Dataloading Functionalities"},{"location":"extending/#adding-training-functionality","text":"For changes at the level of single training, validation, or test steps, modify the specific functions of a given module. For changes at the level of the entire training loop, modify the Training Manager . The main training loop is handled via Trainer class of Pytorch Lightning - please refer to the Pytorch Lightning documentation for more details.","title":"Adding Training Functionality"},{"location":"extending/#adding-inference-functionality","text":"For changes at the level of single inference steps, modify the specific functions of a given module. Note that for inference, special dataloaders are used to load the data in the required format. For changes at the level of the entire inference loop, modify the Inference Manager . The main inference loop is handled via Trainer class of Pytorch Lightning - please refer to the Pytorch Lightning documentation for more details.","title":"Adding Inference Functionality"},{"location":"extending/#adding-new-cli-command","text":"Example: gandlf-synth run CLI command - Implement function and wrap it with @click.command() + @click.option() - Add it to cli_subommands dict The command would be available under gandlf-synth your-subcommand-name CLI command.","title":"Adding new CLI command"},{"location":"extending/#update-parameters","text":"For any new feature that is configurable via config, please ensure the corresponding option in the \"extending\" section of this documentation is added, so that others can review/use/extend it as needed.","title":"Update parameters"},{"location":"extending/#update-tests","text":"Once you have made changes to functionality, it is imperative that the unit tests be updated to cover the new code. Please see the full testing suite for details and examples. Note that tests are split into different categories, each having its own file in the aforementioned folder: - test_modules.py : module-specific tests - test_generic.py : global features tests - entrypoints/ : tests for specific CLI commands","title":"Update Tests"},{"location":"extending/#run-tests","text":"","title":"Run Tests"},{"location":"extending/#prerequisites","text":"Tests are using sample data , which gets downloaded and prepared automatically when you run unit tests. Prepared data is stored at GaNDLF-Synth/testing/data/ automatically the first time test are ran. However, you may want to download & explore data by yourself.","title":"Prerequisites"},{"location":"extending/#unit-tests","text":"Once you have the virtual environment set up, tests can be run using the following command: # continue from previous shell ( venv_gandlf ) $> pytest --device cuda # can be cuda or cpu, defaults to cpu Any failures will be reported in the file GaNDLF-Synth/testing/failures.log .","title":"Unit tests"},{"location":"extending/#code-coverage","text":"The code coverage for the unit tests can be obtained by the following command: # continue from previous shell ( venv_gandlf ) $> coverage run -m pytest --device cuda ; coverage report -m","title":"Code coverage"},{"location":"faq/","text":"Coming soon \u00b6","title":"FAQ"},{"location":"faq/#coming-soon","text":"","title":"Coming soon"},{"location":"getting_started/","text":"This document will help you get started with GaNDLF-Synth using a few representative examples. Installation \u00b6 Follow the installation instructions to install GaNDLF-Synth. When the installation is complete, you should end up with the following shell, which indicates that the GaNDLF-Synth virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here Sample Data \u00b6 Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . An example is shown below: # continue from previous shell ( venv_gandlf ) $> gdown https://drive.google.com/uc?id = 12utErBXZiO_0hspmzUlAQKlN9u-manH_ -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_SYNTH_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo 2d_rad 3d_rad # each of these directories contains data for a specific task in given labeling paradigm Note : When using your own data, it is vital to correctly prepare the data. You can find the details on how to do it using GaNDLF core API here . Train and use models \u00b6 Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sake of this document, we will use 3D radiology images in unlabeled mode, but the same steps can be followed for other modalities and labeling paradigms. For the sample data for this task, the base location is ${GANDLF_SYNTH_DATA}/3d_rad/unlabeled , and it will be referred to as ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED} in the rest of the document. The CSV should look like the example below: Channel_0,Channel_1 ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/003/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/003/t1.nii.gz ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/001/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/001/t1.nii.gz ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/002/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/002/t1.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. You can use any model suitable for this task. An example file for this task can be found here . 4. Now you are ready to train your model . 5. Once the model is trained, you can use it to generate new images (or perform image-to-image reconstruction if you choose suitable model, such as VQVAE). Add inference configuration to the configuration file and run the inference.","title":"Getting Started"},{"location":"getting_started/#installation","text":"Follow the installation instructions to install GaNDLF-Synth. When the installation is complete, you should end up with the following shell, which indicates that the GaNDLF-Synth virtual environment has been activated: ( venv_gandlf ) $> ### subsequent commands go here","title":"Installation"},{"location":"getting_started/#sample-data","text":"Sample data will be used for our extensive automated unit tests in all examples. You can download the sample data from this link . An example is shown below: # continue from previous shell ( venv_gandlf ) $> gdown https://drive.google.com/uc?id = 12utErBXZiO_0hspmzUlAQKlN9u-manH_ -O ./gandlf_sample_data.zip ( venv_gandlf ) $> unzip ./gandlf_sample_data.zip # this should extract a directory called `data` in the current directory The data directory content should look like the example below (for brevity, these locations shall be referred to as ${GANDLF_SYNTH_DATA} in the rest of the document): # continue from previous shell ( venv_gandlf ) $> ls data 2d_histo 2d_rad 3d_rad # each of these directories contains data for a specific task in given labeling paradigm Note : When using your own data, it is vital to correctly prepare the data. You can find the details on how to do it using GaNDLF core API here .","title":"Sample Data"},{"location":"getting_started/#train-and-use-models","text":"Download and extract the sample data as described in the sample data . Alternatively, you can use your own data (see constructing CSV in usage for an example). Construct the main data file that will be used for the entire computation cycle. For the sake of this document, we will use 3D radiology images in unlabeled mode, but the same steps can be followed for other modalities and labeling paradigms. For the sample data for this task, the base location is ${GANDLF_SYNTH_DATA}/3d_rad/unlabeled , and it will be referred to as ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED} in the rest of the document. The CSV should look like the example below: Channel_0,Channel_1 ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/003/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/003/t1.nii.gz ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/001/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/001/t1.nii.gz ${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/002/t2w.nii.gz,${GANDLF_SYNTH_DATA_3DRAD_UNLABELED}/002/t1.nii.gz 3. Construct the configuration file to help design the computation (training and inference) pipeline. You can use any model suitable for this task. An example file for this task can be found here . 4. Now you are ready to train your model . 5. Once the model is trained, you can use it to generate new images (or perform image-to-image reconstruction if you choose suitable model, such as VQVAE). Add inference configuration to the configuration file and run the inference.","title":"Train and use models"},{"location":"setup/","text":"Setup/Installation Instructions \u00b6 Prerequisites \u00b6 Python3 with a preference for conda , and python version 3.9 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . We are working on supporting containerized versions of GaNDLF-Synth, which will be available soon. Optional Requirements \u00b6 GPU compute (usually STRONGLY recommended for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain . Installation \u00b6 Install PyTorch \u00b6 GaNDLF-Synth primary computational foundation is built on PyTorch and PyTorch Lightning, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF-Synth. The version to use needs to be analogous with the GaNDLF version in the setup.py file . For example, for a requirement of gandlf==0.1.1 , the PyTorch requirement is 2.3.1 . See the PyTorch installation instructions for more details. First, instantiate your environment ( base ) $> conda create -n venv_gandlf python = 3 .11 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here You may install PyTorch to be compatible with CUDA, ROCm, or CPU-only. An exhaustive list of PyTorch installations for the specific version compatible with GaNDLF can be found here: https://pytorch.org/get-started/previous-versions/#v231 Use one of the following installation commands provided under the \"Install PyTorch\" section of the PyTorch website. The following example is for installing PyTorch 2.3.1 with CUDA 12.1: - CUDA 12.1 ( venv_gandlf ) $> pip install torch == 2 .3.1 torchvision == 0 .18.1 torchaudio == 2 .3.1 --index-url https://download.pytorch.org/whl/cu121 ### Install from Package Managers This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. ```bash (venv_gandlf) $> pip install gandlf-synth # this will give you the latest stable release You can also use conda ( venv_gandlf ) $> conda install -c conda-forge gandlf-synth -y Or install directly from the GitHub repository ( venv_gandlf ) $> git clone git@github.com:mlcommons/GaNDLF-Synth.git ( venv_gandlf ) $> cd GaNDLF-Synth ( venv_gandlf ) $> pip install . If you are interested in running the latest version of GaNDLF-Synth, you can install the nightly build by running the following command: ( venv_gandlf ) $> pip install --pre gandlf-synth You can also use conda ( venv_gandlf ) $> conda install -c conda-forge/label/gandlf_synth_dev -c conda-forge gandlf-synth -y Test your installation: ( venv_gandlf ) $> gandlf-synth verify-install","title":"Installation"},{"location":"setup/#setupinstallation-instructions","text":"","title":"Setup/Installation Instructions"},{"location":"setup/#prerequisites","text":"Python3 with a preference for conda , and python version 3.9 (higher versions might work, but they are untested ). Knowledge of managing Python environments . The instructions below assume knowledge of the conda management system . We are working on supporting containerized versions of GaNDLF-Synth, which will be available soon.","title":"Prerequisites"},{"location":"setup/#optional-requirements","text":"GPU compute (usually STRONGLY recommended for faster training): Install appropriate drivers: NVIDIA AMD Compute toolkit appropriate for your hardware: NVIDIA: CUDA and a compatible cuDNN installed system-wide AMD: ROCm Windows: Microsoft Visual C++ 14.0 or greater . This is required for PyTorch to work on Windows. If you are using conda, you can install it using the following command for your virtual environment: conda install -c anaconda m2w64-toolchain .","title":"Optional Requirements"},{"location":"setup/#installation","text":"","title":"Installation"},{"location":"setup/#install-pytorch","text":"GaNDLF-Synth primary computational foundation is built on PyTorch and PyTorch Lightning, and as such it supports all hardware types that PyTorch supports. Please install PyTorch for your hardware type before installing GaNDLF-Synth. The version to use needs to be analogous with the GaNDLF version in the setup.py file . For example, for a requirement of gandlf==0.1.1 , the PyTorch requirement is 2.3.1 . See the PyTorch installation instructions for more details. First, instantiate your environment ( base ) $> conda create -n venv_gandlf python = 3 .11 -y ( base ) $> conda activate venv_gandlf ( venv_gandlf ) $> ### subsequent commands go here You may install PyTorch to be compatible with CUDA, ROCm, or CPU-only. An exhaustive list of PyTorch installations for the specific version compatible with GaNDLF can be found here: https://pytorch.org/get-started/previous-versions/#v231 Use one of the following installation commands provided under the \"Install PyTorch\" section of the PyTorch website. The following example is for installing PyTorch 2.3.1 with CUDA 12.1: - CUDA 12.1 ( venv_gandlf ) $> pip install torch == 2 .3.1 torchvision == 0 .18.1 torchaudio == 2 .3.1 --index-url https://download.pytorch.org/whl/cu121 ### Install from Package Managers This option is recommended for most users, and allows for the quickest way to get started with GaNDLF. ```bash (venv_gandlf) $> pip install gandlf-synth # this will give you the latest stable release You can also use conda ( venv_gandlf ) $> conda install -c conda-forge gandlf-synth -y Or install directly from the GitHub repository ( venv_gandlf ) $> git clone git@github.com:mlcommons/GaNDLF-Synth.git ( venv_gandlf ) $> cd GaNDLF-Synth ( venv_gandlf ) $> pip install . If you are interested in running the latest version of GaNDLF-Synth, you can install the nightly build by running the following command: ( venv_gandlf ) $> pip install --pre gandlf-synth You can also use conda ( venv_gandlf ) $> conda install -c conda-forge/label/gandlf_synth_dev -c conda-forge gandlf-synth -y Test your installation: ( venv_gandlf ) $> gandlf-synth verify-install","title":"Install PyTorch"},{"location":"usage/","text":"Introduction \u00b6 To train generative DL models, the usual workflow consists of the following steps: Prepare the data Split data into training, validation, and testing Customize the training parameters Train the model Perform inference GaNDLF-Synth supports all of these steps, but some are optional depending on the nature of the generation task. For example, sometimes it's preferred to avoid splitting the data, and instead use the whole dataset for training. GaNDLF-Synth provides the necessary tools to perform these tasks, using both custom features and the ones provided by GaNDLF. We describe all the functionalities in the following sections. For more details on the functionalities specific to GaNDLF, refer to the GaNDLF documentation . Installation \u00b6 Please follow the installation instructions to install GaNDLF-Synth. Preparing the Data \u00b6 Constructing the Data CSV \u00b6 GaNDLF-Synth can leverage multiple channels/modalities for training using a multi-class segmentation file. The expected format is shown in example CSVs in samples directory for both labeled and unlabeled data. The CSV file needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed): Unlabeled Data \u00b6 Channel_0,Channel_1,...,Channel_X $ROOT-PATH-TO-DATA-FOLDER/1/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/1/2.nii.gz,..., $ROOT-PATH-TO-DATA-FOLDER/2/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/2/2.nii.gz,..., $ROOT-PATH-TO-DATA-FOLDER/3/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/3/2.nii.gz,..., ... Labeled Data \u00b6 Channel_0,Channel_1,Label,LabelMapping $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/1/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/1/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/2/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/2/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/3/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/3/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/1/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/1/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/2/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/2/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/3/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/3/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 ... Notes: For labeled data, the CSV needs to have additonal columns to include the labels corresponding to a given set of channels. It also needs a column for the label mapping, showing the class name assigned to the label value. Using the gandlf-synth construct-csv command \u00b6 To make the process of creating the CSV easier, we have provided a gandlf-synth construct-csv command. The data has to be arranged in different formats, depending on the labeling paradigm. Modality names are used as examples. Unlabeled Data \u00b6 $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_002 \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 ... | \u2514\u2500\u2500\u2500JaneDoe # Patient name can be different \u2502 \u2502 randomFileName_0_t1.nii.gz \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz \u2502 ... Custom Class Labeled Data \u00b6 $DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Class_1 # the class name can be different \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_001 \u2502 \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_002 \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2514\u2500\u2500\u2500Class_2 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_003 \u2502 \u2502 \u2502 Patient_003_brain_t1.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_t1ce.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_t2.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_flair.nii.gz \u2502 \u2502 ... | \u2514\u2500\u2500\u2500Class_N \u2502 \u2514\u2500\u2500\u2500Patient_M \u2502 Patient_M_brain_t1.nii.gz \u2502 Patient_M_brain_t1ce.nii.gz \u2502 Patient_M_brain_t2.nii.gz \u2502 Patient_M_brain_flair.nii.gz Per Patient Labeled Data \u00b6 The folder structure is the same as that of unlabeled data. In the csv, the labels are assigned per patient. The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf-synth construct-csv \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -ch _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for structural brain MR sequences for BraTS, and can be changed based on your data. In the simplest case of a single modality, a \".nii.gz\" will suffice -l 'unlabeled' \\ # labeling paradigm, can be 'unlabeled', 'patient', or 'custom' -o ./experiment_0/train_data.csv # output CSV to be used for training Customize the Training \u00b6 Adapting GaNDLF to your needs boils down to modifying a YAML-based configuration file which controls the parameters of training and inference. Below is a list of available samples for users to start as their baseline for further customization: DDPM (unlabeled paradigm) VQVAE (unlabeled paradigm) Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train. Running GaNDLF-Synth (Training/Inference) \u00b6 You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> gandlf-synth run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -dt ./experiment_0/train.csv \\ # main data CSV used for training (or inference if performing image-to-image reconstruction) -m-dir ./experiment_0/model_dir/ \\ # model directory where the output of the training will be stored, created if not present -t \\ # enable training (if not enabled, inference is performed) # -v-csv ./experiment_0/val.csv \\ # [optional] validation data CSV (if the model performs validation step) # -t-csv ./experiment_0/test.csv \\ # [optional] testing data CSV (if the model performs testing step) # -vr 0.1 \\ # [optional] ratio of validation data to extract from the training data CSV. If -v-csv flag is set, this is ignored # -tr 0.1 \\ # [optional] ratio of testing data to extract from the training data CSV. If -t-csv flag is set, this is ignored # -i-dir ./experiment_0/inference_dir/ \\ # [optional] inference directory where the output of the inference will be stored, created if not present. Used only if inference is enabled # -ckpt-path ./experiment_0/model_dir/checkpoint.ckpt \\ # [optional] path to the checkpoint file to resume training from or to use for inference. If not provided, the latest (or best) checkpoint is used when resuming training or performing inference # -rt , --reset # [optional] completely resets the previous run by deleting `model-dir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `model-dir` Parallelize the Training and Inference \u00b6 Using single or multiple GPUs \u00b6 GaNDLF-Synth supports using single or multiple GPUs out of the box. By default, if the GPU is available ( CUDA_VISIBLE_DEVICES is set), training and inference will use it. If multiple GPUs are available, GaNDLF-Synth will use all of them by DDP strategy (described below). Using Distributed Strategies \u00b6 We currently support DDP and DeepSpeed . To use ddp, just configure the number of nodes and type strategy name \"ddp\" under \"compute\" field in the config. compute : num_devices : 2 # if not set, all GPUs available will num_nodes : 2 # if not set, one node training is assumed strategy : \"ddp\" strategy_config : {} # additional strategy specific kwargs, see https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy For deepspeed, we leverage the original deepspeed library config to set the distributed parameters. To use deepspeed , configure the compute field as follows: compute : num_devices : 2 # if not set, all GPUs available will num_nodes : 2 # if not set, one node training is assumed strategy : \"deepspeed\" strategy_config : config : \"path-to-deepspeed-config.json\" # path to the deepspeed config file Details of this config file can be found in the deepspeed documentation here: https://www.deepspeed.ai/docs/config-json/ Please read further details in the Lightning guide: https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/deepspeed.html#custom-deepspeed-config. Note that you will probably need to override the optimizer choice with one of optimized ones available in deepspeed . This optimizer can be set in the .json config of the strategy (scheduler can be specified here too) and will take precedence over the one specified in the base yaml config file. Mixed precision training: \u00b6 We currently support mixed precision training based on lightning . To use mixed precision, please set the \"precision\" field in the \"compute\" field. All available precision options can be found under the link above. compute : precision : \"16\" Some models (like VQVAE) may not support mixed precision training, so please check the model documentation before enabling it. Expected Output(s) \u00b6 Training \u00b6 Once your model is trained, you should see the following outputin the model directory: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ checkpoints/ # directory containing all the checkpoints training_logs/ # directory containing all the training logs eval_images/ # optionally created - if model was configured to periodically save evaluation images after N epochs (only for 2D runs) parameters.pkl # the used configuration file training_manager.log # global log file of the training manager If you performed inference, the inference directory will contain the following: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/inference_dir/ inference_output/ # directory containing all the inference output (synthesized images) ingerence_logs/ # directory containing all the inference logs from the trainer inference_manager.log # global log file of the inference manager","title":"Usage"},{"location":"usage/#introduction","text":"To train generative DL models, the usual workflow consists of the following steps: Prepare the data Split data into training, validation, and testing Customize the training parameters Train the model Perform inference GaNDLF-Synth supports all of these steps, but some are optional depending on the nature of the generation task. For example, sometimes it's preferred to avoid splitting the data, and instead use the whole dataset for training. GaNDLF-Synth provides the necessary tools to perform these tasks, using both custom features and the ones provided by GaNDLF. We describe all the functionalities in the following sections. For more details on the functionalities specific to GaNDLF, refer to the GaNDLF documentation .","title":"Introduction"},{"location":"usage/#installation","text":"Please follow the installation instructions to install GaNDLF-Synth.","title":"Installation"},{"location":"usage/#preparing-the-data","text":"","title":"Preparing the Data"},{"location":"usage/#constructing-the-data-csv","text":"GaNDLF-Synth can leverage multiple channels/modalities for training using a multi-class segmentation file. The expected format is shown in example CSVs in samples directory for both labeled and unlabeled data. The CSV file needs to be structured with the following header format (which shows a CSV with N subjects, each having X channels/modalities that need to be processed):","title":"Constructing the Data CSV"},{"location":"usage/#unlabeled-data","text":"Channel_0,Channel_1,...,Channel_X $ROOT-PATH-TO-DATA-FOLDER/1/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/1/2.nii.gz,..., $ROOT-PATH-TO-DATA-FOLDER/2/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/2/2.nii.gz,..., $ROOT-PATH-TO-DATA-FOLDER/3/1.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/3/2.nii.gz,..., ...","title":"Unlabeled Data"},{"location":"usage/#labeled-data","text":"Channel_0,Channel_1,Label,LabelMapping $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/1/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/1/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/2/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/2/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/3/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-1/3/t1.nii.gz,0,$CLASS-FOLDER-NAME-1 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/1/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/1/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/2/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/2/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 $ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/3/t2w.nii.gz,$ROOT-PATH-TO-DATA-FOLDER/$CLASS-FOLDER-NAME-2/3/t1.nii.gz,1,$CLASS-FOLDER-NAME-2 ... Notes: For labeled data, the CSV needs to have additonal columns to include the labels corresponding to a given set of channels. It also needs a column for the label mapping, showing the class name assigned to the label value.","title":"Labeled Data"},{"location":"usage/#using-the-gandlf-synth-construct-csv-command","text":"To make the process of creating the CSV easier, we have provided a gandlf-synth construct-csv command. The data has to be arranged in different formats, depending on the labeling paradigm. Modality names are used as examples.","title":"Using the gandlf-synth construct-csv command"},{"location":"usage/#unlabeled-data_1","text":"$DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Patient_001 \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_002 \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 ... | \u2514\u2500\u2500\u2500JaneDoe # Patient name can be different \u2502 \u2502 randomFileName_0_t1.nii.gz \u2502 \u2502 randomFileName_1_t1ce.nii.gz \u2502 \u2502 randomFileName_2_t2.nii.gz \u2502 \u2502 randomFileName_3_flair.nii.gz \u2502 \u2502 randomFileName_seg.nii.gz \u2502 ...","title":"Unlabeled Data"},{"location":"usage/#custom-class-labeled-data","text":"$DATA_DIRECTORY \u2502 \u2514\u2500\u2500\u2500Class_1 # the class name can be different \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_001 \u2502 \u2502 \u2502 Patient_001_brain_t1.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_t1ce.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_t2.nii.gz \u2502 \u2502 \u2502 Patient_001_brain_flair.nii.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_002 \u2502 \u2502 Patient_002_brain_t1.nii.gz \u2502 \u2502 Patient_002_brain_t1ce.nii.gz \u2502 \u2502 Patient_002_brain_t2.nii.gz \u2502 \u2502 Patient_002_brain_flair.nii.gz \u2502 \u2514\u2500\u2500\u2500Class_2 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500Patient_003 \u2502 \u2502 \u2502 Patient_003_brain_t1.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_t1ce.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_t2.nii.gz \u2502 \u2502 \u2502 Patient_003_brain_flair.nii.gz \u2502 \u2502 ... | \u2514\u2500\u2500\u2500Class_N \u2502 \u2514\u2500\u2500\u2500Patient_M \u2502 Patient_M_brain_t1.nii.gz \u2502 Patient_M_brain_t1ce.nii.gz \u2502 Patient_M_brain_t2.nii.gz \u2502 Patient_M_brain_flair.nii.gz","title":"Custom Class Labeled Data"},{"location":"usage/#per-patient-labeled-data","text":"The folder structure is the same as that of unlabeled data. In the csv, the labels are assigned per patient. The following command shows how the script works: # continue from previous shell ( venv_gandlf ) $> gandlf-synth construct-csv \\ # -h, --help Show help message and exit -i $DATA_DIRECTORY # this is the main data directory -ch _t1.nii.gz,_t1ce.nii.gz,_t2.nii.gz,_flair.nii.gz \\ # an example image identifier for structural brain MR sequences for BraTS, and can be changed based on your data. In the simplest case of a single modality, a \".nii.gz\" will suffice -l 'unlabeled' \\ # labeling paradigm, can be 'unlabeled', 'patient', or 'custom' -o ./experiment_0/train_data.csv # output CSV to be used for training","title":"Per Patient Labeled Data"},{"location":"usage/#customize-the-training","text":"Adapting GaNDLF to your needs boils down to modifying a YAML-based configuration file which controls the parameters of training and inference. Below is a list of available samples for users to start as their baseline for further customization: DDPM (unlabeled paradigm) VQVAE (unlabeled paradigm) Notes : More details on the configuration options are available in the customization page . Ensure that the configuration has valid syntax by checking the file using any YAML validator such as yamlchecker.com or yamlvalidator.com before trying to train.","title":"Customize the Training"},{"location":"usage/#running-gandlf-synth-traininginference","text":"You can use the following code snippet to run GaNDLF: # continue from previous shell ( venv_gandlf ) $> gandlf-synth run \\ # -h, --help Show help message and exit # -v, --version Show program's version number and exit. -c ./experiment_0/model.yaml \\ # model configuration - needs to be a valid YAML (check syntax using https://yamlchecker.com/) -dt ./experiment_0/train.csv \\ # main data CSV used for training (or inference if performing image-to-image reconstruction) -m-dir ./experiment_0/model_dir/ \\ # model directory where the output of the training will be stored, created if not present -t \\ # enable training (if not enabled, inference is performed) # -v-csv ./experiment_0/val.csv \\ # [optional] validation data CSV (if the model performs validation step) # -t-csv ./experiment_0/test.csv \\ # [optional] testing data CSV (if the model performs testing step) # -vr 0.1 \\ # [optional] ratio of validation data to extract from the training data CSV. If -v-csv flag is set, this is ignored # -tr 0.1 \\ # [optional] ratio of testing data to extract from the training data CSV. If -t-csv flag is set, this is ignored # -i-dir ./experiment_0/inference_dir/ \\ # [optional] inference directory where the output of the inference will be stored, created if not present. Used only if inference is enabled # -ckpt-path ./experiment_0/model_dir/checkpoint.ckpt \\ # [optional] path to the checkpoint file to resume training from or to use for inference. If not provided, the latest (or best) checkpoint is used when resuming training or performing inference # -rt , --reset # [optional] completely resets the previous run by deleting `model-dir` # -rm , --resume # [optional] resume previous training by only keeping model dict in `model-dir`","title":"Running GaNDLF-Synth (Training/Inference)"},{"location":"usage/#parallelize-the-training-and-inference","text":"","title":"Parallelize the Training and Inference"},{"location":"usage/#using-single-or-multiple-gpus","text":"GaNDLF-Synth supports using single or multiple GPUs out of the box. By default, if the GPU is available ( CUDA_VISIBLE_DEVICES is set), training and inference will use it. If multiple GPUs are available, GaNDLF-Synth will use all of them by DDP strategy (described below).","title":"Using single or multiple GPUs"},{"location":"usage/#using-distributed-strategies","text":"We currently support DDP and DeepSpeed . To use ddp, just configure the number of nodes and type strategy name \"ddp\" under \"compute\" field in the config. compute : num_devices : 2 # if not set, all GPUs available will num_nodes : 2 # if not set, one node training is assumed strategy : \"ddp\" strategy_config : {} # additional strategy specific kwargs, see https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.DDPStrategy.html#lightning.pytorch.strategies.DDPStrategy For deepspeed, we leverage the original deepspeed library config to set the distributed parameters. To use deepspeed , configure the compute field as follows: compute : num_devices : 2 # if not set, all GPUs available will num_nodes : 2 # if not set, one node training is assumed strategy : \"deepspeed\" strategy_config : config : \"path-to-deepspeed-config.json\" # path to the deepspeed config file Details of this config file can be found in the deepspeed documentation here: https://www.deepspeed.ai/docs/config-json/ Please read further details in the Lightning guide: https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/deepspeed.html#custom-deepspeed-config. Note that you will probably need to override the optimizer choice with one of optimized ones available in deepspeed . This optimizer can be set in the .json config of the strategy (scheduler can be specified here too) and will take precedence over the one specified in the base yaml config file.","title":"Using Distributed Strategies"},{"location":"usage/#mixed-precision-training","text":"We currently support mixed precision training based on lightning . To use mixed precision, please set the \"precision\" field in the \"compute\" field. All available precision options can be found under the link above. compute : precision : \"16\" Some models (like VQVAE) may not support mixed precision training, so please check the model documentation before enabling it.","title":"Mixed precision training:"},{"location":"usage/#expected-outputs","text":"","title":"Expected Output(s)"},{"location":"usage/#training","text":"Once your model is trained, you should see the following outputin the model directory: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/model_dir/ checkpoints/ # directory containing all the checkpoints training_logs/ # directory containing all the training logs eval_images/ # optionally created - if model was configured to periodically save evaluation images after N epochs (only for 2D runs) parameters.pkl # the used configuration file training_manager.log # global log file of the training manager If you performed inference, the inference directory will contain the following: # continue from previous shell ( venv_gandlf ) $> ls ./experiment_0/inference_dir/ inference_output/ # directory containing all the inference output (synthesized images) ingerence_logs/ # directory containing all the inference logs from the trainer inference_manager.log # global log file of the inference manager","title":"Training"}]}